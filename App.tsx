import React, { useState, useRef, useCallback, useEffect } from 'react';
import JSZip from 'jszip';
import { generateSpeech } from './services/geminiService';
import { decode, decodeAudioData, createWavBlob, audioBufferToPcm } from './utils/audioUtils';
import { AVAILABLE_VOICES } from './constants';
import type { AudioChunk } from './types';
import { Header } from './components/Header';
import { VoiceSelector } from './components/VoiceSelector';
import { ActionButton } from './components/ActionButton';
import { AudioControls } from './components/AudioControls';
import { AudioChunkList } from './components/AudioChunkList';

const App: React.FC = () => {
  const [text, setText] = useState<string>('Hello! I am a friendly voice generated by Gemini. You can type any text here and I will read it for you. For longer texts, I will automatically split them into smaller chunks and stitch them back together for seamless playback.');
  const [style, setStyle] = useState<string>('');
  const [selectedVoice, setSelectedVoice] = useState<string>(AVAILABLE_VOICES[0].id);
  const [audioChunks, setAudioChunks] = useState<AudioChunk[]>([]);
  const [playingChunkId, setPlayingChunkId] = useState<number | null>(null);
  const [isPlayingGlobal, setIsPlayingGlobal] = useState<boolean>(false); 
  const [volume, setVolume] = useState<number>(1);
  const [playbackTime, setPlaybackTime] = useState<number>(0);

  // Refs
  const audioContextRef = useRef<AudioContext | null>(null);
  const chunkBuffersRef = useRef<Map<number, AudioBuffer>>(new Map());
  const audioSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const gainNodeRef = useRef<GainNode | null>(null);
  const cancelledChunksRef = useRef<Set<number>>(new Set());
  
  // For timeline tracking
  const startTimeRef = useRef<number>(0);
  const animationFrameRef = useRef<number>(0);

  const CHARACTER_LIMIT = 1500;

  // Initialize AudioContext
  const initAudioContext = () => {
    if (!audioContextRef.current) {
      // @ts-ignore
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
      gainNodeRef.current = audioContextRef.current.createGain();
      gainNodeRef.current.gain.value = volume;
      gainNodeRef.current.connect(audioContextRef.current.destination);
    } else if (audioContextRef.current.state === 'suspended') {
      audioContextRef.current.resume();
    }
    return audioContextRef.current;
  };

  const stopPlayback = useCallback(() => {
    if (audioSourceRef.current) {
      try {
        audioSourceRef.current.stop();
        audioSourceRef.current.disconnect();
      } catch (e) {
        // Ignore errors if already stopped
      }
      audioSourceRef.current = null;
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
    setPlayingChunkId(null);
    setIsPlayingGlobal(false);
    setPlaybackTime(0);
  }, []);

  // Helper to process a single chunk (API Call + Decoding)
  const processChunk = async (chunkId: number, chunkText: string, voice: string, styleInstruction: string) => {
    try {
      // Update status to generating
      setAudioChunks(prev => prev.map(c => c.id === chunkId ? { ...c, status: 'generating', error: undefined } : c));

      const result = await generateSpeech(chunkText, voice, styleInstruction);
      
      // Check cancellation before heavy processing
      if (cancelledChunksRef.current.has(chunkId)) {
        setAudioChunks(prev => prev.map(c => c.id === chunkId ? { ...c, status: 'idle' } : c));
        return;
      }

      const decodedPcm = decode(result);
      
      const ctx = initAudioContext();
      const audioBuffer = await decodeAudioData(decodedPcm, ctx, 24000, 1);
      
      // Store buffer in ref
      chunkBuffersRef.current.set(chunkId, audioBuffer);

      // Final check for cancellation
      if (cancelledChunksRef.current.has(chunkId)) {
         setAudioChunks(prev => prev.map(c => c.id === chunkId ? { ...c, status: 'idle' } : c));
         return;
      }

      // Update status to completed
      setAudioChunks(prev => prev.map(c => 
        c.id === chunkId 
          ? { ...c, status: 'completed', audioData: result, duration: audioBuffer.duration, isSelected: true } 
          : c
      ));

    } catch (err) {
      console.error(`Error processing chunk ${chunkId}:`, err);
      if (!cancelledChunksRef.current.has(chunkId)) {
        setAudioChunks(prev => prev.map(c => 
          c.id === chunkId 
            ? { ...c, status: 'error', error: err instanceof Error ? err.message : 'Failed' } 
            : c
        ));
      }
    }
  };

  const handleGenerateSpeech = async () => {
    if (!text.trim()) return;
    
    stopPlayback();
    setAudioChunks([]);
    chunkBuffersRef.current.clear();
    cancelledChunksRef.current.clear();

    // Split text logic
    const chunksText: string[] = [];
    let currentText = text.trim();

    if (currentText.length <= CHARACTER_LIMIT) {
      chunksText.push(currentText);
    } else {
      while (currentText.length > 0) {
          if (currentText.length <= CHARACTER_LIMIT) {
              chunksText.push(currentText);
              break;
          }
          let sliceEnd = -1;
          const sentenceEndings = ['.', '?', '!'];
          let bestSentenceEnd = -1;
          for (const ending of sentenceEndings) {
              const index = currentText.lastIndexOf(ending, CHARACTER_LIMIT);
              if (index > bestSentenceEnd) {
                  bestSentenceEnd = index;
              }
          }
          if (bestSentenceEnd !== -1) {
              sliceEnd = bestSentenceEnd + 1;
          } else {
              const spaceEnd = currentText.lastIndexOf(' ', CHARACTER_LIMIT);
              sliceEnd = spaceEnd !== -1 ? spaceEnd + 1 : CHARACTER_LIMIT;
          }
          chunksText.push(currentText.substring(0, sliceEnd).trim());
          currentText = currentText.substring(sliceEnd).trim();
      }
    }

    // Initialize chunk objects
    const newChunks: AudioChunk[] = chunksText.map((t, i) => ({
      id: i,
      text: t,
      status: 'idle',
      audioData: null,
      duration: 0,
      isSelected: false
    }));
    
    setAudioChunks(newChunks);

    // Trigger parallel processing
    newChunks.forEach(chunk => {
      processChunk(chunk.id, chunk.text, selectedVoice, style);
    });
  };

  const handleRetryChunk = (id: number) => {
    if (playingChunkId === id) {
      stopPlayback();
    }
    cancelledChunksRef.current.delete(id);
    const chunk = audioChunks.find(c => c.id === id);
    if (chunk) {
      processChunk(id, chunk.text, selectedVoice, style);
    }
  };

  const handleCancelChunk = (id: number) => {
    cancelledChunksRef.current.add(id);
    setAudioChunks(prev => prev.map(c => c.id === id ? { ...c, status: 'idle', error: 'Cancelled' } : c));
  };

  // Animation loop for updating timeline
  const updateTimeline = useCallback(() => {
    if (!audioContextRef.current || !startTimeRef.current) return;
    const currentTime = audioContextRef.current.currentTime - startTimeRef.current;
    setPlaybackTime(currentTime);
    animationFrameRef.current = requestAnimationFrame(updateTimeline);
  }, []);

  const playSpecificChunk = useCallback((id: number, startOffset: number = 0) => {
    const buffer = chunkBuffersRef.current.get(id);
    if (!buffer) return;

    const ctx = initAudioContext();
    stopPlayback(); // Stop anything currently playing

    const source = ctx.createBufferSource();
    source.buffer = buffer;
    source.connect(gainNodeRef.current!);
    
    source.onended = () => {
       setPlayingChunkId(prev => (prev === id ? null : prev));
       cancelAnimationFrame(animationFrameRef.current);
    };

    audioSourceRef.current = source;
    source.start(0, startOffset);
    
    // Setup timeline tracking
    startTimeRef.current = ctx.currentTime - startOffset;
    setPlayingChunkId(id);
    setPlaybackTime(startOffset);
    animationFrameRef.current = requestAnimationFrame(updateTimeline);

  }, [stopPlayback, updateTimeline]);

  const handleSeek = (time: number) => {
    if (playingChunkId !== null) {
        playSpecificChunk(playingChunkId, time);
    }
  };

  const pausePlayback = useCallback(() => {
    if (audioSourceRef.current) {
      audioSourceRef.current.stop();
      setPlayingChunkId(null);
      setIsPlayingGlobal(false);
      if (animationFrameRef.current) cancelAnimationFrame(animationFrameRef.current);
    }
  }, []);

  // Global Play (Stitch Completed)
  const concatenateAudioBuffers = useCallback((buffers: AudioBuffer[]): AudioBuffer | null => {
    if (!buffers.length || !audioContextRef.current) return null;
    const totalLength = buffers.reduce((acc, buffer) => acc + buffer.length, 0);
    const numChannels = buffers[0].numberOfChannels;
    const sampleRate = buffers[0].sampleRate;
    const result = audioContextRef.current.createBuffer(numChannels, totalLength, sampleRate);
    let offset = 0;
    for (const buffer of buffers) {
      for (let channel = 0; channel < numChannels; channel++) {
        result.getChannelData(channel).set(buffer.getChannelData(channel), offset);
      }
      offset += buffer.length;
    }
    return result;
  }, []);

  const playAllCompleted = useCallback(() => {
    const completedChunks = audioChunks.filter(c => c.status === 'completed').sort((a, b) => a.id - b.id);
    if (completedChunks.length === 0) return;

    const buffers = completedChunks.map(c => chunkBuffersRef.current.get(c.id)).filter(Boolean) as AudioBuffer[];
    const fullBuffer = concatenateAudioBuffers(buffers);
    
    if (!fullBuffer) return;

    const ctx = initAudioContext();
    stopPlayback();

    const source = ctx.createBufferSource();
    source.buffer = fullBuffer;
    source.connect(gainNodeRef.current!);
    
    source.onended = () => {
      setIsPlayingGlobal(false);
    };

    audioSourceRef.current = source;
    source.start();
    setIsPlayingGlobal(true);
  }, [audioChunks, concatenateAudioBuffers, stopPlayback]);


  const handleVolumeChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    const newVolume = parseFloat(event.target.value);
    setVolume(newVolume);
    if (gainNodeRef.current && audioContextRef.current) {
      gainNodeRef.current.gain.setValueAtTime(newVolume, audioContextRef.current.currentTime);
    }
  };

  const downloadWav = (buffer: AudioBuffer, filename: string) => {
    try {
        const pcmData = audioBufferToPcm(buffer);
        const wavBlob = createWavBlob(pcmData, 24000, 1);
        const url = URL.createObjectURL(wavBlob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
    } catch(e) {
        console.error("Download error:", e);
    }
  };

  const handleDownloadAll = () => {
     const completedChunks = audioChunks.filter(c => c.status === 'completed').sort((a, b) => a.id - b.id);
     const buffers = completedChunks.map(c => chunkBuffersRef.current.get(c.id)).filter(Boolean) as AudioBuffer[];
     const fullBuffer = concatenateAudioBuffers(buffers);
     if (fullBuffer) downloadWav(fullBuffer, 'gemini-speech-full.wav');
  };

  const handleDownloadSelected = async () => {
    const selectedChunks = audioChunks.filter(c => c.isSelected && c.status === 'completed').sort((a, b) => a.id - b.id);
    if (selectedChunks.length === 0) return;
    
    // If only one file, download directly as WAV
    if (selectedChunks.length === 1) {
        const chunk = selectedChunks[0];
        const buffer = chunkBuffersRef.current.get(chunk.id);
        if (buffer) {
            downloadWav(buffer, `gemini-speech-part-${chunk.id + 1}.wav`);
        }
        return;
    }

    // If multiple files, zip them
    const zip = new JSZip();
    
    selectedChunks.forEach((chunk) => {
        const buffer = chunkBuffersRef.current.get(chunk.id);
        if (buffer) {
            const pcmData = audioBufferToPcm(buffer);
            const wavBlob = createWavBlob(pcmData, 24000, 1);
            zip.file(`gemini-speech-part-${chunk.id + 1}.wav`, wavBlob);
        }
    });

    try {
        const content = await zip.generateAsync({ type: "blob" });
        const url = URL.createObjectURL(content);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = "gemini-speech-selected.zip";
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
    } catch (e) {
        console.error("Error generating zip:", e);
    }
  };

  const handleToggleChunkSelection = (id: number) => {
    setAudioChunks(chunks =>
      chunks.map(chunk =>
        chunk.id === id ? { ...chunk, isSelected: !chunk.isSelected } : chunk
      )
    );
  };

  const handleToggleSelectAll = (checked: boolean) => {
    setAudioChunks(chunks => chunks.map(chunk => ({ ...chunk, isSelected: checked })));
  };

  const isGeneratingAny = audioChunks.some(c => c.status === 'generating');
  const hasCompleted = audioChunks.some(c => c.status === 'completed');

  return (
    <div className="min-h-screen bg-slate-900 font-sans p-4 sm:p-6 lg:p-8 flex flex-col items-center">
      <div className="w-full max-w-2xl mx-auto">
        <Header />
        <main className="mt-8 p-6 bg-slate-800/50 rounded-2xl shadow-2xl border border-slate-700 backdrop-blur-sm">
          <div className="space-y-2">
            <textarea
              value={text}
              onChange={(e) => setText(e.target.value)}
              placeholder="Enter text to convert to speech..."
              className="w-full h-40 p-4 bg-slate-900/70 border border-slate-600 rounded-lg focus:ring-2 focus:ring-sky-500 focus:border-sky-500 outline-none transition-all duration-300 resize-none text-slate-200 placeholder-slate-500"
              disabled={isGeneratingAny}
              aria-label="Text to Speech Input"
            />
            <div className="text-right text-sm text-slate-400 pr-2">
                <span className={text.length > CHARACTER_LIMIT ? 'text-amber-400 font-bold' : ''}>
                    {text.length}
                </span> / {CHARACTER_LIMIT}
                {text.length > CHARACTER_LIMIT && <span className="text-amber-400 text-xs"> (Audio will be split)</span>}
            </div>
             <input
              type="text"
              value={style}
              onChange={(e) => setStyle(e.target.value)}
              placeholder="Optional: Enter style instructions (e.g., 'speak cheerfully')"
              className="w-full p-3 bg-slate-900/70 border border-slate-600 rounded-lg focus:ring-2 focus:ring-sky-500 focus:border-sky-500 outline-none transition-all duration-300 text-slate-200 placeholder-slate-500"
              disabled={isGeneratingAny}
              aria-label="Style Instructions"
            />
            <VoiceSelector
              voices={AVAILABLE_VOICES}
              selectedValue={selectedVoice}
              onChange={(e) => setSelectedVoice(e.target.value)}
              disabled={isGeneratingAny}
            />
          </div>

          <div className="mt-6 text-center">
            <ActionButton onClick={handleGenerateSpeech} disabled={!text.trim() || isGeneratingAny}>
              {isGeneratingAny ? 'Generating...' : 'Generate Speech'}
            </ActionButton>
          </div>

          {/* Global Controls (Play All / Stop All) */}
          {hasCompleted && (
            <AudioControls
              isPlaying={isPlayingGlobal}
              onPlayPause={() => isPlayingGlobal ? stopPlayback() : playAllCompleted()}
              onStop={stopPlayback}
              onDownload={handleDownloadAll}
              volume={volume}
              onVolumeChange={handleVolumeChange}
            />
          )}

          {/* Individual Chunks List */}
          {audioChunks.length > 0 && (
            <AudioChunkList
              chunks={audioChunks}
              playingChunkId={playingChunkId}
              playbackTime={playbackTime}
              onToggleSelection={handleToggleChunkSelection}
              onToggleSelectAll={handleToggleSelectAll}
              onDownloadSelected={handleDownloadSelected}
              onPlayChunk={playSpecificChunk}
              onPauseChunk={pausePlayback}
              onRetryChunk={handleRetryChunk}
              onCancelChunk={handleCancelChunk}
              onSeek={handleSeek}
              hasSelection={audioChunks.some(c => c.isSelected)}
            />
          )}
        </main>
        <footer className="text-center mt-8 text-slate-500 text-sm">
          <p>Powered by Google Gemini. Built with React & Tailwind CSS.</p>
        </footer>
      </div>
    </div>
  );
};

export default App;